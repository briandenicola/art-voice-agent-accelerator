{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Framework Playground\n",
    "\n",
    "Interactive notebook for evaluating agent performance with the cascade orchestrator.\n",
    "\n",
    "## Features\n",
    "\n",
    "- ‚úÖ Test YAML scenario loading\n",
    "- ‚úÖ Run orchestrator turns with full evaluation\n",
    "- ‚úÖ Record events and score performance\n",
    "- ‚úÖ Compare model configurations (GPT-4o vs o1, different verbosity levels)\n",
    "- ‚úÖ A/B testing capabilities\n",
    "\n",
    "## Quick Links\n",
    "\n",
    "- [Evaluation Package](../../../apps/artagent/backend/evaluation/)\n",
    "- [Documentation](../../../docs/testing/model-evals.md)\n",
    "- [Test Scenarios](../../../tests/eval_scenarios/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Project Root: /Users/jinle/Repos/_AIProjects/art-voice-agent-accelerator\n",
      "‚úÖ Python path configured\n",
      "\n",
      "‚úÖ Python path configured\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import asyncio\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import yaml\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Add project root to Python path\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent.parent.parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"üìÅ Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"‚úÖ Python path configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loading .env.local\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   App Config (appconfig-contoso-z8kttnsm): 50 keys synced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded configuration from Azure App Config (appconfig-contoso-z8kttnsm)\n",
      "\n",
      "üìã Configuration source: Azure App Config (appconfig-contoso-z8kttnsm)\n",
      "‚úÖ Azure OpenAI endpoint: https://artagentz8kttnsmaif.openai.azure.com/\n",
      "‚úÖ Default deployment: gpt-4o\n",
      "\n",
      "\n",
      "üìã Configuration source: Azure App Config (appconfig-contoso-z8kttnsm)\n",
      "‚úÖ Azure OpenAI endpoint: https://artagentz8kttnsmaif.openai.azure.com/\n",
      "‚úÖ Default deployment: gpt-4o\n"
     ]
    }
   ],
   "source": [
    "# Load environment configuration\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env first (fallback)\n",
    "env_local_path = PROJECT_ROOT / \".env.local\"\n",
    "env_path = PROJECT_ROOT / \".env\"\n",
    "\n",
    "if env_local_path.exists():\n",
    "    print(f\"‚úÖ Loading .env.local\")\n",
    "    load_dotenv(env_local_path, override=True)\n",
    "    config_source = \".env.local\"\n",
    "elif env_path.exists():\n",
    "    print(f\"‚úÖ Loading .env\")\n",
    "    load_dotenv(env_path, override=True)\n",
    "    config_source = \".env\"\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No .env file found. Using system environment variables.\")\n",
    "    config_source = \"system environment\"\n",
    "\n",
    "# Try to load Azure App Configuration (preferred)\n",
    "try:\n",
    "    from apps.artagent.backend.config.appconfig_provider import bootstrap_appconfig, get_provider_status\n",
    "    \n",
    "    appconfig_loaded = bootstrap_appconfig()\n",
    "    if appconfig_loaded:\n",
    "        status = get_provider_status()\n",
    "        endpoint_name = status.get(\"endpoint\", \"\").split(\"//\")[-1].split(\".\")[0] if status.get(\"endpoint\") else \"unknown\"\n",
    "        print(f\"‚úÖ Loaded configuration from Azure App Config ({endpoint_name})\")\n",
    "        config_source = f\"Azure App Config ({endpoint_name})\"\n",
    "except Exception as e:\n",
    "    print(f\"‚ÑπÔ∏è  App Configuration not available, using {config_source}\")\n",
    "\n",
    "# Verify Azure OpenAI is configured\n",
    "endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "deployment = os.getenv('AZURE_OPENAI_CHAT_DEPLOYMENT_ID') or 'gpt-4o'\n",
    "\n",
    "print(f\"\\nüìã Configuration source: {config_source}\")\n",
    "if endpoint:\n",
    "    print(f\"‚úÖ Azure OpenAI endpoint: {endpoint}\")\n",
    "else:\n",
    "    print(\"‚ùå AZURE_OPENAI_ENDPOINT not set\")\n",
    "\n",
    "if deployment:\n",
    "    print(f\"‚úÖ Default deployment: {deployment}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  AZURE_OPENAI_DEPLOYMENT_NAME not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All components imported successfully\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import evaluation framework\n",
    "from apps.artagent.backend.evaluation import (\n",
    "    EventRecorder,\n",
    "    EvaluationOrchestratorWrapper,\n",
    "    MetricsScorer,\n",
    "    ComparisonRunner,\n",
    "    MockMemoManager,\n",
    "    build_context,\n",
    ")\n",
    "\n",
    "# Import orchestrator components\n",
    "from apps.artagent.backend.registries.agentstore.loader import (\n",
    "    discover_agents,\n",
    "    get_agent,\n",
    "    build_handoff_map,\n",
    ")\n",
    "from apps.artagent.backend.registries.agentstore.base import ModelConfig\n",
    "from apps.artagent.backend.voice.speech_cascade.orchestrator import (\n",
    "    CascadeOrchestratorAdapter,\n",
    ")\n",
    "from apps.artagent.backend.voice.shared.base import OrchestratorContext\n",
    "\n",
    "print(\"‚úÖ All components imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Discover Available Agents\n",
    "\n",
    "Load real agents from the agent registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Discovered 12 agents:\n",
      "\n",
      "Banking Agents:\n",
      "  ‚Ä¢ AuthAgent: Handles MFA, identity verification, and security questions\n",
      "  ‚Ä¢ BankingConcierge: Primary banking assistant - handles most customer needs and routes complex reque\n",
      "  ‚Ä¢ CardRecommendation: Credit card recommendations, comparisons, and e-signature applications\n",
      "  ‚Ä¢ ComplianceDesk: AML, FATCA, sanctions screening, and regulatory compliance verification\n",
      "  ‚Ä¢ FraudAgent: Post-authentication fraud detection specialist handling credit card fraud,\n",
      "ident\n",
      "  ‚Ä¢ InvestmentAdvisor: Retirement accounts, 401(k) rollovers, IRA guidance, investment products\n",
      "\n",
      "Insurance Agents:\n",
      "  ‚Ä¢ ClaimsSpecialist: Insurance claims specialist who helps customers file, track, and manage their in\n",
      "  ‚Ä¢ FNOLAgent: Insurance First-Notice-of-Loss (FNOL) intake specialist. Collects claim\n",
      "informat\n",
      "  ‚Ä¢ PolicyAdvisor: Insurance policy advisor who helps customers with policy changes, renewals, and \n",
      "  ‚Ä¢ SubroAgent: Subrogation specialist handling B2B calls from Claimant Carriers (other insuranc\n",
      "\n",
      "Other Agents:\n",
      "  ‚Ä¢ Concierge: Primary  assistant - handles most customer needs and routes complex requests to \n",
      "  ‚Ä¢ GeneralKBAgent: Knowledge base assistant for general inquiries, FAQs, and product information - \n",
      "\n",
      "\n",
      "üîó Handoff map: 11 handoff triggers configured\n",
      "\n",
      "Banking Agents:\n",
      "  ‚Ä¢ AuthAgent: Handles MFA, identity verification, and security questions\n",
      "  ‚Ä¢ BankingConcierge: Primary banking assistant - handles most customer needs and routes complex reque\n",
      "  ‚Ä¢ CardRecommendation: Credit card recommendations, comparisons, and e-signature applications\n",
      "  ‚Ä¢ ComplianceDesk: AML, FATCA, sanctions screening, and regulatory compliance verification\n",
      "  ‚Ä¢ FraudAgent: Post-authentication fraud detection specialist handling credit card fraud,\n",
      "ident\n",
      "  ‚Ä¢ InvestmentAdvisor: Retirement accounts, 401(k) rollovers, IRA guidance, investment products\n",
      "\n",
      "Insurance Agents:\n",
      "  ‚Ä¢ ClaimsSpecialist: Insurance claims specialist who helps customers file, track, and manage their in\n",
      "  ‚Ä¢ FNOLAgent: Insurance First-Notice-of-Loss (FNOL) intake specialist. Collects claim\n",
      "informat\n",
      "  ‚Ä¢ PolicyAdvisor: Insurance policy advisor who helps customers with policy changes, renewals, and \n",
      "  ‚Ä¢ SubroAgent: Subrogation specialist handling B2B calls from Claimant Carriers (other insuranc\n",
      "\n",
      "Other Agents:\n",
      "  ‚Ä¢ Concierge: Primary  assistant - handles most customer needs and routes complex requests to \n",
      "  ‚Ä¢ GeneralKBAgent: Knowledge base assistant for general inquiries, FAQs, and product information - \n",
      "\n",
      "\n",
      "üîó Handoff map: 11 handoff triggers configured\n"
     ]
    }
   ],
   "source": [
    "# Discover all available agents\n",
    "agents = discover_agents()\n",
    "handoff_map = build_handoff_map(agents)\n",
    "\n",
    "print(f\"üì¶ Discovered {len(agents)} agents:\\n\")\n",
    "\n",
    "# Group by category\n",
    "banking = []\n",
    "insurance = []\n",
    "other = []\n",
    "\n",
    "for name, agent in sorted(agents.items()):\n",
    "    desc = agent.description[:80] if agent.description else \"No description\"\n",
    "    \n",
    "    # Categorize based on name patterns\n",
    "    if any(x in name.lower() for x in ['banking', 'fraud', 'investment', 'card', 'auth', 'compliance']):\n",
    "        banking.append((name, desc))\n",
    "    elif any(x in name.lower() for x in ['claims', 'policy', 'fnol', 'subro']):\n",
    "        insurance.append((name, desc))\n",
    "    else:\n",
    "        other.append((name, desc))\n",
    "\n",
    "if banking:\n",
    "    print(\"Banking Agents:\")\n",
    "    for name, desc in banking:\n",
    "        print(f\"  ‚Ä¢ {name}: {desc}\")\n",
    "    print()\n",
    "\n",
    "if insurance:\n",
    "    print(\"Insurance Agents:\")\n",
    "    for name, desc in insurance:\n",
    "        print(f\"  ‚Ä¢ {name}: {desc}\")\n",
    "    print()\n",
    "\n",
    "if other:\n",
    "    print(\"Other Agents:\")\n",
    "    for name, desc in other:\n",
    "        print(f\"  ‚Ä¢ {name}: {desc}\")\n",
    "    print()\n",
    "\n",
    "print(f\"\\nüîó Handoff map: {len(handoff_map)} handoff triggers configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Custom Cascade Orchestrator instance\n",
    "\n",
    "This creates an actual orchestrator with real agents (not mocks):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ create_orchestrator() function defined\n",
      "   - o-series/gpt-5: max_completion_tokens, NO temperature\n",
      "   - gpt-4.1: max_completion_tokens, with temperature\n",
      "   - gpt-4o: max_tokens + temperature (legacy)\n",
      "\n",
      "You can now create orchestrators with:\n",
      "  orchestrator = create_orchestrator('FraudAgent')\n",
      "\n",
      "   - o-series/gpt-5: max_completion_tokens, NO temperature\n",
      "   - gpt-4.1: max_completion_tokens, with temperature\n",
      "   - gpt-4o: max_tokens + temperature (legacy)\n",
      "\n",
      "You can now create orchestrators with:\n",
      "  orchestrator = create_orchestrator('FraudAgent')\n"
     ]
    }
   ],
   "source": [
    "def create_orchestrator(\n",
    "    agent_name: str,\n",
    "    model_override: dict = None,\n",
    "    session_id: str = \"eval-session\",\n",
    ") -> CascadeOrchestratorAdapter:\n",
    "    \"\"\"\n",
    "    Create a real orchestrator for evaluation.\n",
    "    \n",
    "    Args:\n",
    "        agent_name: Name of the agent to use\n",
    "        model_override: Optional model configuration override\n",
    "        session_id: Session ID for tracking\n",
    "    \n",
    "    Returns:\n",
    "        Configured CascadeOrchestratorAdapter\n",
    "    \"\"\"\n",
    "    # Load all agents\n",
    "    all_agents = discover_agents()\n",
    "    \n",
    "    # Apply model override if provided\n",
    "    if model_override and agent_name in all_agents:\n",
    "        agent = all_agents[agent_name]\n",
    "        deployment_id = model_override.get('deployment_id', agent.model.deployment_id)\n",
    "        \n",
    "        # Detect model family from deployment_id\n",
    "        deployment_lower = deployment_id.lower()\n",
    "        \n",
    "        # Models that use max_completion_tokens instead of max_tokens\n",
    "        uses_max_completion_tokens = any(x in deployment_lower for x in [\n",
    "            'o1', 'o3', 'o4',           # Reasoning models\n",
    "            'gpt-5', 'gpt5',            # GPT-5 family\n",
    "            'gpt-4.1', 'gpt4.1',        # GPT-4.1 family\n",
    "        ])\n",
    "        \n",
    "        # Models that don't support custom temperature (only default=1)\n",
    "        # o-series: reasoning models\n",
    "        # gpt-5: only supports default temperature\n",
    "        no_custom_temperature = any(x in deployment_lower for x in [\n",
    "            'o1', 'o3', 'o4',           # Reasoning models\n",
    "            'gpt-5', 'gpt5',            # GPT-5 family (only default temp)\n",
    "        ])\n",
    "        \n",
    "        # For new-gen models: use max_completion_tokens, not max_tokens\n",
    "        if uses_max_completion_tokens:\n",
    "            max_tokens_val = None  # New models don't support max_tokens\n",
    "            max_completion_tokens_val = model_override.get(\n",
    "                'max_completion_tokens', \n",
    "                model_override.get('max_tokens', 4096)  # Allow fallback from max_tokens\n",
    "            )\n",
    "        else:\n",
    "            max_tokens_val = model_override.get('max_tokens', agent.model.max_tokens)\n",
    "            max_completion_tokens_val = model_override.get('max_completion_tokens')\n",
    "        \n",
    "        # Temperature handling\n",
    "        if no_custom_temperature:\n",
    "            temperature_val = None  # These models only support default (1)\n",
    "        else:\n",
    "            temperature_val = model_override.get('temperature', agent.model.temperature)\n",
    "        \n",
    "        # Reasoning effort only for o-series\n",
    "        is_reasoning_model = any(x in deployment_lower for x in ['o1', 'o3', 'o4'])\n",
    "        reasoning_effort = model_override.get('reasoning_effort', 'medium') if is_reasoning_model else model_override.get('reasoning_effort')\n",
    "        \n",
    "        # Create new model config with proper parameters for model type\n",
    "        model_config = ModelConfig(\n",
    "            deployment_id=deployment_id,\n",
    "            endpoint_preference=model_override.get('endpoint_preference', agent.model.endpoint_preference),\n",
    "            verbosity=model_override.get('verbosity', agent.model.verbosity),\n",
    "            temperature=temperature_val,\n",
    "            max_tokens=max_tokens_val,\n",
    "            max_completion_tokens=max_completion_tokens_val,\n",
    "            reasoning_effort=reasoning_effort,\n",
    "        )\n",
    "        \n",
    "        # Update agent's model config\n",
    "        agent.model = model_config\n",
    "        agent.cascade_model = model_config\n",
    "    \n",
    "    # Build handoff map\n",
    "    handoff_map = build_handoff_map(all_agents)\n",
    "    \n",
    "    # Create orchestrator\n",
    "    orchestrator = CascadeOrchestratorAdapter.create(\n",
    "        start_agent=agent_name,\n",
    "        session_id=session_id,\n",
    "        call_connection_id=f\"eval-{session_id}\",\n",
    "        agents=all_agents,\n",
    "        handoff_map=handoff_map,\n",
    "        enable_rag=False,  # Disable RAG for faster eval\n",
    "        streaming=False,   # Non-streaming for eval\n",
    "    )\n",
    "    \n",
    "    return orchestrator\n",
    "\n",
    "print(\"‚úÖ create_orchestrator() function defined\")\n",
    "print(\"   - o-series/gpt-5: max_completion_tokens, NO temperature\")\n",
    "print(\"   - gpt-4.1: max_completion_tokens, with temperature\")\n",
    "print(\"   - gpt-4o: max_tokens + temperature (legacy)\")\n",
    "print(\"\\nYou can now create orchestrators with:\")\n",
    "print(\"  orchestrator = create_orchestrator('FraudAgent')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run a Single Turn (Real Agent!)\n",
    "\n",
    "Test a real agent with a real query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ run_single_turn() function defined\n",
      "   Evaluates agent with full orchestrator execution\n",
      "\n",
      "   Evaluates agent with full orchestrator execution\n"
     ]
    }
   ],
   "source": [
    "async def run_single_turn(\n",
    "    agent_name: str,\n",
    "    user_query: str,\n",
    "    model_override: dict = None,\n",
    "    record_events: bool = True,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Run a single turn with the cascade orchestrator.\n",
    "    \n",
    "    Args:\n",
    "        agent_name: Agent to use\n",
    "        user_query: User's question/request\n",
    "        model_override: Optional model config override\n",
    "        record_events: Whether to record events\n",
    "    \n",
    "    Returns:\n",
    "        Result dictionary with response and metrics\n",
    "    \"\"\"\n",
    "    import time\n",
    "    from apps.artagent.backend.evaluation import MockMemoManager\n",
    "    \n",
    "    session_id = f\"eval-{int(time.time())}\"\n",
    "    \n",
    "    # Create MemoManager for session state\n",
    "    memo_manager = MockMemoManager(\n",
    "        session_id=session_id,\n",
    "        context={\"caller_name\": \"Test User\", \"client_id\": \"test_client\"}\n",
    "    )\n",
    "    \n",
    "    # Create orchestrator\n",
    "    orchestrator = create_orchestrator(\n",
    "        agent_name=agent_name,\n",
    "        model_override=model_override,\n",
    "        session_id=session_id,\n",
    "    )\n",
    "    \n",
    "    # DEBUG: Verify model config is correctly applied on the orchestrator's agent\n",
    "    if model_override:\n",
    "        # Get the agent from the orchestrator's internal state (not a fresh discover)\n",
    "        agent = orchestrator.agents.get(agent_name)\n",
    "        if agent:\n",
    "            actual_model = agent.get_model_for_mode(\"cascade\")\n",
    "            print(f\"  ‚û°Ô∏è  Model config applied: deployment={actual_model.deployment_id}, \"\n",
    "                  f\"verbosity={actual_model.verbosity}, \"\n",
    "                  f\"endpoint_pref={actual_model.endpoint_preference}\")\n",
    "    \n",
    "    # Optionally wrap with recorder\n",
    "    if record_events:\n",
    "        recorder = EventRecorder(\n",
    "            run_id=f\"{agent_name}_{session_id}\",\n",
    "            output_dir=PROJECT_ROOT / \"runs\" / \"jupyter_tests\",\n",
    "        )\n",
    "        orchestrator = EvaluationOrchestratorWrapper(\n",
    "            orchestrator=orchestrator,\n",
    "            recorder=recorder,\n",
    "        )\n",
    "    \n",
    "    # Create context WITH MemoManager!\n",
    "    context = OrchestratorContext(\n",
    "        session_id=session_id,\n",
    "        user_text=user_query,\n",
    "        turn_id=\"turn_1\",\n",
    "        conversation_history=memo_manager.get_history(agent_name),\n",
    "        metadata={\n",
    "            \"scenario\": \"jupyter_test\",\n",
    "            \"memo_manager\": memo_manager,  # ‚Üê THE KEY!\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    # Run turn through orchestrator\n",
    "    start_time = time.time()\n",
    "    result = await orchestrator.process_turn(context)\n",
    "    elapsed_ms = (time.time() - start_time) * 1000\n",
    "    \n",
    "    # Update history\n",
    "    memo_manager.append_to_history(agent_name, \"user\", user_query)\n",
    "    memo_manager.append_to_history(agent_name, \"assistant\", result.response_text)\n",
    "    \n",
    "    return {\n",
    "        \"query\": user_query,\n",
    "        \"agent\": agent_name,\n",
    "        \"response\": result.response_text,\n",
    "        \"model\": model_override.get('deployment_id') if model_override else 'default',\n",
    "        \"endpoint\": model_override.get('endpoint_preference') if model_override else 'auto',\n",
    "        \"verbosity\": model_override.get('verbosity') if model_override else 'default',\n",
    "        \"input_tokens\": result.input_tokens,\n",
    "        \"output_tokens\": result.output_tokens,\n",
    "        \"latency_ms\": elapsed_ms,\n",
    "        \"error\": result.error,\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ run_single_turn() function defined\")\n",
    "print(\"   Evaluates agent with full orchestrator execution\")\n",
    "print(\"   Now includes verbosity debugging output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test It! Run a Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-02 13:56:11,907] INFO - apps.artagent.backend.evaluation.recorder: EventRecorder initialized | run_id=FraudAgent_eval-1767383771 output=/Users/jinle/Repos/_AIProjects/art-voice-agent-accelerator/runs/jupyter_tests/FraudAgent_eval-1767383771_events.jsonl\n",
      "[2026-01-02 13:56:11,909] INFO - apps.artagent.backend.evaluation.wrappers: EvaluationOrchestratorWrapper initialized | orchestrator=CascadeOrchestratorAdapter recorder=FraudAgent_eval-1767383771\n",
      "[2026-01-02 13:56:11,919] INFO - voice.shared.config_resolver: Checking for session-scoped scenario | session_id=eval-1767383771 scenario_name=None\n",
      "[2026-01-02 13:56:11,921] INFO - voice.shared.config_resolver: No stored scenarios for session | session_id=eval-1767383771\n",
      "[2026-01-02 13:56:11,970] INFO - cascade.adapter: üîß Agent tools loaded | agent=FraudAgent tool_count=12 tool_names=['analyze_recent_transactions', 'check_suspicious_activity', 'block_card_emergency', 'create_fraud_case', 'create_transaction_dispute', 'ship_replacement_card', 'send_fraud_case_email', 'provide_fraud_education', 'search_knowledge_base', 'transfer_call_to_call_center', 'escalate_emergency', 'escalate_human']\n",
      "[2026-01-02 13:56:12,009] INFO - cascade.adapter: Starting LLM request (streaming) | agent=FraudAgent model=gpt-4o temp=0.7 iteration=0 tools=12\n",
      "[2026-01-02 13:56:13,086] INFO - cascade.adapter: LLM response (streamed) | agent=FraudAgent text_len=37 tool_calls=1 (filtered from 1) iteration=0\n",
      "[2026-01-02 13:56:13,088] INFO - agents.tools.fraud: üîç Fraud analysis: customer_id - risk_score: 0.85\n",
      "[2026-01-02 13:56:13,090] INFO - cascade.adapter: Tool executed | name=analyze_recent_transactions result_keys=['success', 'analysis']\n",
      "[2026-01-02 13:56:13,133] INFO - cascade.adapter: Starting LLM request (streaming) | agent=FraudAgent model=gpt-4o temp=0.7 iteration=1 tools=12\n",
      "[2026-01-02 13:56:13,926] INFO - cascade.adapter: LLM response (streamed) | agent=FraudAgent text_len=84 tool_calls=0 (filtered from 0) iteration=1\n",
      "[2026-01-02 13:56:13,931] INFO - apps.artagent.backend.evaluation.recorder: Turn end | turn_id=turn_1767383771 agent=FraudAgent e2e=2019.2ms tools=1 response_len=84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üéØ EVALUATION RESULT\n",
      "======================================================================\n",
      "\n",
      "üìù Query: I see a $500 charge from Amazon that I didn't make\n",
      "üéØ EVALUATION RESULT\n",
      "======================================================================\n",
      "\n",
      "üìù Query: I see a $500 charge from Amazon that I didn't make\n",
      "\n",
      "ü§ñ Agent: FraudAgent\n",
      "üí¨ Response:\n",
      "I found the charge and other flagged activity. Would you like me to block your card?\n",
      "\n",
      "üìä Metrics:\n",
      "  ‚Ä¢ Model: gpt-4o\n",
      "  ‚Ä¢ Endpoint: chat\n",
      "  ‚Ä¢ Input tokens: 0\n",
      "  ‚Ä¢ Output tokens: 30\n",
      "  ‚Ä¢ Latency: 2022ms\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example: Evaluate FraudAgent with GPT-4o\n",
    "result = await run_single_turn(\n",
    "    agent_name=\"FraudAgent\",\n",
    "    user_query=\"I see a $500 charge from Amazon that I didn't make\",\n",
    "    model_override={\n",
    "        \"deployment_id\": \"gpt-4o\",\n",
    "        \"endpoint_preference\": \"responses\",  # Use responses API for verbosity support\n",
    "        \"verbosity\": 0,  # Minimal verbosity for faster responses\n",
    "        \"temperature\": 0.7,\n",
    "    },\n",
    "    record_events=True,\n",
    ")\n",
    "\n",
    "# Display result\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéØ EVALUATION RESULT\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nüìù Query: {result['query']}\")\n",
    "print(f\"\\nü§ñ Agent: {result['agent']}\")\n",
    "print(f\"üí¨ Response:\\n{result['response']}\")\n",
    "print(f\"\\nüìä Metrics:\")\n",
    "print(f\"  ‚Ä¢ Model: {result['model']}\")\n",
    "print(f\"  ‚Ä¢ Endpoint: {result['endpoint']}\")\n",
    "print(f\"  ‚Ä¢ Verbosity: {result['verbosity']}\")\n",
    "print(f\"  ‚Ä¢ Input tokens: {result['input_tokens']}\")\n",
    "print(f\"  ‚Ä¢ Output tokens: {result['output_tokens']}\")\n",
    "print(f\"  ‚Ä¢ Latency: {result['latency_ms']:.0f}ms\")\n",
    "if result['error']:\n",
    "    print(f\"  ‚Ä¢ ‚ùå Error: {result['error']}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Model Configurations (i.e gpt-5-mini vs gpt-4o from the above)\n",
    "\n",
    "Test the same query with different model configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing gpt-5-nano with responses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-02 14:04:04,374] INFO - apps.artagent.backend.evaluation.recorder: EventRecorder initialized | run_id=FraudAgent_eval-1767384244 output=/Users/jinle/Repos/_AIProjects/art-voice-agent-accelerator/runs/jupyter_tests/FraudAgent_eval-1767384244_events.jsonl\n",
      "[2026-01-02 14:04:04,380] INFO - apps.artagent.backend.evaluation.wrappers: EvaluationOrchestratorWrapper initialized | orchestrator=CascadeOrchestratorAdapter recorder=FraudAgent_eval-1767384244\n",
      "[2026-01-02 14:04:04,397] INFO - voice.shared.config_resolver: Checking for session-scoped scenario | session_id=eval-1767384244 scenario_name=None\n",
      "[2026-01-02 14:04:04,400] INFO - voice.shared.config_resolver: No stored scenarios for session | session_id=eval-1767384244\n",
      "[2026-01-02 14:04:04,526] INFO - cascade.adapter: üîß Agent tools loaded | agent=FraudAgent tool_count=12 tool_names=['analyze_recent_transactions', 'check_suspicious_activity', 'block_card_emergency', 'create_fraud_case', 'create_transaction_dispute', 'ship_replacement_card', 'send_fraud_case_email', 'provide_fraud_education', 'search_knowledge_base', 'transfer_call_to_call_center', 'escalate_emergency', 'escalate_human']\n",
      "[2026-01-02 14:04:04,580] INFO - cascade.adapter: Starting LLM request (streaming) | agent=FraudAgent model=gpt-5-nano temp=N/A iteration=0 tools=12\n",
      "[2026-01-02 14:04:14,158] INFO - cascade.adapter: LLM response (streamed) | agent=FraudAgent text_len=0 tool_calls=1 (filtered from 1) iteration=0\n",
      "[2026-01-02 14:04:14,162] INFO - agents.tools.fraud: üîç Fraud analysis: current_customer - risk_score: 0.85\n",
      "[2026-01-02 14:04:14,164] INFO - cascade.adapter: Tool executed | name=analyze_recent_transactions result_keys=['success', 'analysis']\n",
      "[2026-01-02 14:04:14,244] INFO - cascade.adapter: Starting LLM request (streaming) | agent=FraudAgent model=gpt-5-nano temp=N/A iteration=1 tools=12\n",
      "[2026-01-02 14:04:31,558] INFO - cascade.adapter: LLM response (streamed) | agent=FraudAgent text_len=381 tool_calls=0 (filtered from 0) iteration=1\n",
      "[2026-01-02 14:04:31,564] INFO - apps.artagent.backend.evaluation.recorder: Turn end | turn_id=turn_1767384244 agent=FraudAgent e2e=27181.3ms tools=1 response_len=381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing gpt-5-nano with responses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-02 14:04:31,714] INFO - apps.artagent.backend.evaluation.recorder: EventRecorder initialized | run_id=FraudAgent_eval-1767384271 output=/Users/jinle/Repos/_AIProjects/art-voice-agent-accelerator/runs/jupyter_tests/FraudAgent_eval-1767384271_events.jsonl\n",
      "[2026-01-02 14:04:31,717] INFO - apps.artagent.backend.evaluation.wrappers: EvaluationOrchestratorWrapper initialized | orchestrator=CascadeOrchestratorAdapter recorder=FraudAgent_eval-1767384271\n",
      "[2026-01-02 14:04:31,728] INFO - voice.shared.config_resolver: Checking for session-scoped scenario | session_id=eval-1767384271 scenario_name=None\n",
      "[2026-01-02 14:04:31,731] INFO - voice.shared.config_resolver: No stored scenarios for session | session_id=eval-1767384271\n",
      "[2026-01-02 14:04:31,894] INFO - cascade.adapter: üîß Agent tools loaded | agent=FraudAgent tool_count=12 tool_names=['analyze_recent_transactions', 'check_suspicious_activity', 'block_card_emergency', 'create_fraud_case', 'create_transaction_dispute', 'ship_replacement_card', 'send_fraud_case_email', 'provide_fraud_education', 'search_knowledge_base', 'transfer_call_to_call_center', 'escalate_emergency', 'escalate_human']\n",
      "[2026-01-02 14:04:31,940] INFO - cascade.adapter: Starting LLM request (streaming) | agent=FraudAgent model=gpt-5-nano temp=N/A iteration=0 tools=12\n",
      "[2026-01-02 14:04:43,588] INFO - cascade.adapter: LLM response (streamed) | agent=FraudAgent text_len=0 tool_calls=1 (filtered from 1) iteration=0\n",
      "[2026-01-02 14:04:43,590] INFO - agents.tools.fraud: üîç Fraud analysis: current_client - risk_score: 0.85\n",
      "[2026-01-02 14:04:43,592] INFO - cascade.adapter: Tool executed | name=analyze_recent_transactions result_keys=['success', 'analysis']\n",
      "[2026-01-02 14:04:43,635] INFO - cascade.adapter: Starting LLM request (streaming) | agent=FraudAgent model=gpt-5-nano temp=N/A iteration=1 tools=12\n",
      "[2026-01-02 14:04:53,347] INFO - cascade.adapter: LLM response (streamed) | agent=FraudAgent text_len=0 tool_calls=1 (filtered from 1) iteration=1\n",
      "[2026-01-02 14:04:53,350] WARNING - agents.tools.fraud: üö® Card blocked: current_client - ******** - reason: High risk score and suspicious transactions detected; potential card compromise; blocking to prevent further unauthorized charges.\n",
      "[2026-01-02 14:04:53,352] INFO - cascade.adapter: Tool executed | name=block_card_emergency result_keys=['success', 'blocked', 'card_last4', 'blocked_at', 'message', 'next_step']\n",
      "[2026-01-02 14:04:53,392] INFO - cascade.adapter: Starting LLM request (streaming) | agent=FraudAgent model=gpt-5-nano temp=N/A iteration=2 tools=12\n",
      "[2026-01-02 14:05:06,355] INFO - cascade.adapter: LLM response (streamed) | agent=FraudAgent text_len=0 tool_calls=1 (filtered from 1) iteration=2\n",
      "[2026-01-02 14:05:06,361] INFO - agents.tools.fraud: üìù Fraud case created: FRD-20260102-8636 for current_client\n",
      "[2026-01-02 14:05:06,365] INFO - cascade.adapter: Tool executed | name=create_fraud_case result_keys=['success', 'case_id', 'status', 'next_steps', 'reference_number']\n",
      "[2026-01-02 14:05:06,444] INFO - cascade.adapter: Starting LLM request (streaming) | agent=FraudAgent model=gpt-5-nano temp=N/A iteration=3 tools=12\n",
      "[2026-01-02 14:05:17,170] INFO - cascade.adapter: LLM response (streamed) | agent=FraudAgent text_len=0 tool_calls=1 (filtered from 1) iteration=3\n",
      "[2026-01-02 14:05:17,174] INFO - agents.tools.fraud: üìß Fraud case email sent: current_client - case: FRD-20260102-8636\n",
      "[2026-01-02 14:05:17,178] INFO - cascade.adapter: Tool executed | name=send_fraud_case_email result_keys=['success', 'email_sent', 'recipient', 'case_id', 'content_included']\n",
      "[2026-01-02 14:05:17,232] INFO - cascade.adapter: Starting LLM request (streaming) | agent=FraudAgent model=gpt-5-nano temp=N/A iteration=4 tools=12\n",
      "[2026-01-02 14:05:31,634] INFO - cascade.adapter: LLM response (streamed) | agent=FraudAgent text_len=463 tool_calls=0 (filtered from 0) iteration=4\n",
      "[2026-01-02 14:05:31,656] INFO - apps.artagent.backend.evaluation.recorder: Turn end | turn_id=turn_1767384271 agent=FraudAgent e2e=59925.7ms tools=4 response_len=463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing gpt-5-nano with responses...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-02 14:05:31,806] INFO - apps.artagent.backend.evaluation.recorder: EventRecorder initialized | run_id=FraudAgent_eval-1767384331 output=/Users/jinle/Repos/_AIProjects/art-voice-agent-accelerator/runs/jupyter_tests/FraudAgent_eval-1767384331_events.jsonl\n",
      "[2026-01-02 14:05:31,808] INFO - apps.artagent.backend.evaluation.wrappers: EvaluationOrchestratorWrapper initialized | orchestrator=CascadeOrchestratorAdapter recorder=FraudAgent_eval-1767384331\n",
      "[2026-01-02 14:05:31,820] INFO - voice.shared.config_resolver: Checking for session-scoped scenario | session_id=eval-1767384331 scenario_name=None\n",
      "[2026-01-02 14:05:31,822] INFO - voice.shared.config_resolver: No stored scenarios for session | session_id=eval-1767384331\n",
      "[2026-01-02 14:05:31,942] INFO - cascade.adapter: üîß Agent tools loaded | agent=FraudAgent tool_count=12 tool_names=['analyze_recent_transactions', 'check_suspicious_activity', 'block_card_emergency', 'create_fraud_case', 'create_transaction_dispute', 'ship_replacement_card', 'send_fraud_case_email', 'provide_fraud_education', 'search_knowledge_base', 'transfer_call_to_call_center', 'escalate_emergency', 'escalate_human']\n",
      "[2026-01-02 14:05:31,980] INFO - cascade.adapter: Starting LLM request (streaming) | agent=FraudAgent model=gpt-5-nano temp=N/A iteration=0 tools=12\n",
      "[2026-01-02 14:05:35,546] INFO - cascade.adapter: LLM response (streamed) | agent=FraudAgent text_len=0 tool_calls=1 (filtered from 1) iteration=0\n",
      "[2026-01-02 14:05:35,549] INFO - agents.tools.fraud: üîç Fraud analysis: customer_001 - risk_score: 0.92\n",
      "[2026-01-02 14:05:35,553] INFO - cascade.adapter: Tool executed | name=analyze_recent_transactions result_keys=['success', 'analysis']\n",
      "[2026-01-02 14:05:35,620] INFO - cascade.adapter: Starting LLM request (streaming) | agent=FraudAgent model=gpt-5-nano temp=N/A iteration=1 tools=12\n",
      "[2026-01-02 14:05:57,534] INFO - cascade.adapter: LLM response (streamed) | agent=FraudAgent text_len=0 tool_calls=1 (filtered from 1) iteration=1\n",
      "[2026-01-02 14:05:57,541] WARNING - agents.tools.fraud: üö® Card blocked: customer_001 - ******** - reason: High risk flagged transactions indicating possible account compromise; two suspicious charges detected.\n",
      "[2026-01-02 14:05:57,545] INFO - cascade.adapter: Tool executed | name=block_card_emergency result_keys=['success', 'blocked', 'card_last4', 'blocked_at', 'message', 'next_step']\n",
      "[2026-01-02 14:05:57,601] INFO - cascade.adapter: Starting LLM request (streaming) | agent=FraudAgent model=gpt-5-nano temp=N/A iteration=2 tools=12\n",
      "[2026-01-02 14:06:27,523] INFO - cascade.adapter: LLM response (streamed) | agent=FraudAgent text_len=0 tool_calls=0 (filtered from 0) iteration=2\n",
      "[2026-01-02 14:06:27,531] INFO - apps.artagent.backend.evaluation.recorder: Turn end | turn_id=turn_1767384331 agent=FraudAgent e2e=55720.5ms tools=2 response_len=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä MODEL CONFIGURATION COMPARISON\n",
      "======================================================================\n",
      "\n",
      "üìä MODEL CONFIGURATION COMPARISON\n",
      "======================================================================\n",
      "\n",
      "1. gpt-5-nano | responses\n",
      "   Response length: 381 chars\n",
      "   Output tokens: 95\n",
      "   Latency: 27183ms\n",
      "   Response: I found two suspicious charges.\n",
      "\n",
      "The first is five thousand dollars at WIRE TRANSFER INTL. Date: 2024-11-29. Flags: wire_transfer, unusual_amount, fir...\n",
      "\n",
      "2. gpt-5-nano | responses\n",
      "   Response length: 463 chars\n",
      "   Output tokens: 115\n",
      "   Latency: 59940ms\n",
      "   Response: Thanks for telling me. I found two suspicious transactions. One is one thousand eight hundred ninety-nine dollars and ninety-nine cents at ELECTRONICS...\n",
      "\n",
      "3. gpt-5-nano | responses\n",
      "   Response length: 0 chars\n",
      "   Output tokens: 0\n",
      "   Latency: 55723ms\n",
      "   Response: ...\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "async def compare_model_configs(\n",
    "    agent_name: str,\n",
    "    user_query: str,\n",
    "    configs: list[dict],\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Run the same query with different model configurations.\n",
    "    \n",
    "    Args:\n",
    "        agent_name: Agent to test\n",
    "        user_query: Query to test\n",
    "        configs: List of model config dictionaries\n",
    "    \n",
    "    Returns:\n",
    "        List of results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for config in configs:\n",
    "        print(f\"Testing {config.get('deployment_id')} with {config.get('endpoint_preference')}...\")\n",
    "        \n",
    "        result = await run_single_turn(\n",
    "            agent_name=agent_name,\n",
    "            user_query=user_query,\n",
    "            model_override=config,\n",
    "            record_events=True,\n",
    "        )\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example: Compare GPT-4o Chat vs Responses API\n",
    "comparison_results = await compare_model_configs(\n",
    "    agent_name=\"FraudAgent\",\n",
    "    user_query=\"I see a suspicious $500 charge\",\n",
    "    configs=[\n",
    "        {\n",
    "            \"deployment_id\": \"gpt-5-nano\",\n",
    "            \"endpoint_preference\": \"responses\",\n",
    "            \"verbosity\": 1,\n",
    "            \"temperature\": 0.7,\n",
    "        },\n",
    "        {\n",
    "            \"deployment_id\": \"gpt-5-nano\",\n",
    "            \"endpoint_preference\": \"responses\",\n",
    "            \"verbosity\": 0,  # Minimal\n",
    "        },\n",
    "        {\n",
    "            \"deployment_id\": \"gpt-5-nano\",\n",
    "            \"endpoint_preference\": \"responses\",\n",
    "            \"verbosity\": 2,  # Detailed\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Display comparison\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä MODEL CONFIGURATION COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "for i, result in enumerate(comparison_results, 1):\n",
    "    print(f\"\\n{i}. {result['model']} | {result['endpoint']}\")\n",
    "    print(f\"   Response length: {len(result['response'])} chars\")\n",
    "    print(f\"   Output tokens: {result['output_tokens']}\")\n",
    "    print(f\"   Latency: {result['latency_ms']:.0f}ms\")\n",
    "    print(f\"   Response: {result['response'][:150]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostic: Verify Model Parameters\n",
    "\n",
    "Let's verify what parameters are actually being sent to the API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic: Check what parameters are being prepared for the API call\n",
    "from src.aoai.manager import AzureOpenAIManager\n",
    "from apps.artagent.backend.registries.agentstore.base import ModelConfig\n",
    "\n",
    "def diagnose_model_params(model_override: dict):\n",
    "    \"\"\"\n",
    "    Diagnose what parameters would be sent to the API.\n",
    "    \n",
    "    This shows exactly what the AzureOpenAIManager prepares\n",
    "    based on your model_override settings.\n",
    "    \"\"\"\n",
    "    # Create a ModelConfig from the override\n",
    "    config = ModelConfig(\n",
    "        deployment_id=model_override.get('deployment_id', 'gpt-4o'),\n",
    "        endpoint_preference=model_override.get('endpoint_preference', 'auto'),\n",
    "        verbosity=model_override.get('verbosity', 0),\n",
    "        temperature=model_override.get('temperature'),\n",
    "        max_tokens=model_override.get('max_tokens'),\n",
    "        max_completion_tokens=model_override.get('max_completion_tokens'),\n",
    "        reasoning_effort=model_override.get('reasoning_effort'),\n",
    "    )\n",
    "    \n",
    "    print(f\"üìã ModelConfig created:\")\n",
    "    print(f\"   ‚Ä¢ deployment_id: {config.deployment_id}\")\n",
    "    print(f\"   ‚Ä¢ endpoint_preference: {config.endpoint_preference}\")\n",
    "    print(f\"   ‚Ä¢ verbosity: {config.verbosity}\")\n",
    "    print(f\"   ‚Ä¢ temperature: {config.temperature}\")\n",
    "    print(f\"   ‚Ä¢ max_tokens: {config.max_tokens}\")\n",
    "    print(f\"   ‚Ä¢ max_completion_tokens: {config.max_completion_tokens}\")\n",
    "    print(f\"   ‚Ä¢ reasoning_effort: {config.reasoning_effort}\")\n",
    "    \n",
    "    # Create a temporary manager to check parameter preparation\n",
    "    manager = AzureOpenAIManager(enable_tracing=False)\n",
    "    \n",
    "    # Check which endpoint would be used\n",
    "    use_responses = manager._should_use_responses_endpoint(config)\n",
    "    print(f\"\\nüîÄ Endpoint decision: {'RESPONSES' if use_responses else 'CHAT'}\")\n",
    "    \n",
    "    # Sample messages for testing\n",
    "    test_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello\"},\n",
    "    ]\n",
    "    \n",
    "    # Get prepared parameters\n",
    "    if use_responses:\n",
    "        params = manager._prepare_responses_params(config, test_messages)\n",
    "        print(f\"\\nüì¶ Responses API params:\")\n",
    "    else:\n",
    "        params = manager._prepare_chat_params(config, test_messages)\n",
    "        print(f\"\\nüì¶ Chat API params:\")\n",
    "    \n",
    "    # Show key parameters (excluding messages for brevity)\n",
    "    for key, value in params.items():\n",
    "        if key != 'messages' and key != 'input':\n",
    "            print(f\"   ‚Ä¢ {key}: {value}\")\n",
    "    \n",
    "    # Check if verbosity is included\n",
    "    if 'verbosity' in params:\n",
    "        print(f\"\\n‚úÖ verbosity IS being sent to API: {params['verbosity']}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  verbosity is NOT in the API params!\")\n",
    "    \n",
    "    return params\n",
    "\n",
    "# Test with different configurations\n",
    "print(\"=\" * 70)\n",
    "print(\"üîç DIAGNOSTIC: Testing verbosity parameter passing\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n--- Test 1: GPT-4o with Chat endpoint ---\")\n",
    "diagnose_model_params({\n",
    "    \"deployment_id\": \"gpt-4o\",\n",
    "    \"endpoint_preference\": \"chat\",\n",
    "    \"verbosity\": 0,\n",
    "})\n",
    "\n",
    "print(\"\\n--- Test 2: GPT-4o with Responses endpoint ---\")\n",
    "diagnose_model_params({\n",
    "    \"deployment_id\": \"gpt-4o\",\n",
    "    \"endpoint_preference\": \"responses\",\n",
    "    \"verbosity\": 0,\n",
    "})\n",
    "\n",
    "print(\"\\n--- Test 3: GPT-5-mini with Responses endpoint ---\")\n",
    "diagnose_model_params({\n",
    "    \"deployment_id\": \"gpt-5-mini\",\n",
    "    \"endpoint_preference\": \"responses\",\n",
    "    \"verbosity\": 0,\n",
    "    \"reasoning_effort\": \"low\",\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Score Recorded Events\n",
    "\n",
    "Load and score events that were recorded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Found 45 event file(s):\n",
      "\n",
      "  ‚Ä¢ AuthAgent_eval-1767383325_events.jsonl\n",
      "  ‚Ä¢ AuthAgent_eval-1767302973_events.jsonl\n",
      "  ‚Ä¢ FraudAgent_eval-1767382517_events.jsonl\n",
      "  ‚Ä¢ FraudAgent_eval-1767381582_events.jsonl\n",
      "  ‚Ä¢ FraudAgent_eval-1767381654_events.jsonl\n",
      "  ‚Ä¢ FraudAgent_eval-1767381606_events.jsonl\n",
      "  ‚Ä¢ FraudAgent_eval-1767381769_events.jsonl\n",
      "  ‚Ä¢ FraudAgent_eval-1767304683_events.jsonl\n",
      "  ‚Ä¢ AuthAgent_eval-1767305809_events.jsonl\n",
      "  ‚Ä¢ FraudAgent_eval-1767381697_events.jsonl\n",
      "  ‚Ä¢ AuthAgent_eval-1767306630_events.jsonl\n",
      "  ‚Ä¢ FraudAgent_eval-1767304682_events.jsonl\n",
      "  ‚Ä¢ FraudAgent_eval-1767303785_events.jsonl\n",
      "  ‚Ä¢ FraudAgent_eval-1767303782_events.jsonl\n",
      "  ‚Ä¢ FraudAgent_eval-1767382508_events.jsonl\n",
      "  ‚Ä¢ FraudAgent_eval-1767304659_events.jsonl\n",
      "  ‚Ä¢ FraudAgent_eval-1767381759_events.jsonl\n",
      "  ‚Ä¢ FraudAgent_eval-1767383708_events.jsonl\n",
      "  ‚Ä¢ FraudAgent_eval-1767382518_events.jsonl\n",
      "  ‚Ä¢ FraudAgent_eval-1767381809_events.jsonl\n",
      "  ‚Ä¢ FraudAgent_eval-1767303787_events.jsonl\n",
      "  ‚Ä¢ FraudAgent_eval-1767303993_events.jsonl\n",
      "  ‚Ä¢ AuthAgent_eval-1767303006_events.jsonl\n",
      "  ‚Ä¢ AuthAgent_eval-1767304670_events.jsonl\n",
      "  ‚Ä¢ AuthAgent_eval-1767302997_events.jsonl\n",
      "  ‚Ä¢ AuthAgent_eval-1767304762_events.jsonl\n",
      "  ‚Ä¢ FraudAgent_eval-1767303992_events.jsonl\n",
      "  ‚Ä¢ AuthAgent_eval-1767382543_events.jsonl\n",
      "  ‚Ä¢ AuthAgent_eval-1767306638_events.jsonl\n",
      "  ‚Ä¢ FraudAgent_eval-1767304934_events.jsonl\n",
      "  ‚Ä¢ AuthAgent_eval-1767383321_events.jsonl\n",
      "  ‚Ä¢ FraudAgent_eval-1767305838_events.jsonl\n",
      "  ‚Ä¢ FraudAgent_eval-1767303774_events.jsonl\n",
      "  ‚Ä¢ AuthAgent_eval-1767383310_events.jsonl\n",
      "  ‚Ä¢ FraudAgent_eval-1767305849_events.jsonl\n",
      "  ‚Ä¢ FraudAgent_eval-1767304145_events.jsonl\n",
      "  ‚Ä¢ FraudAgent_eval-1767383700_events.jsonl\n",
      "  ‚Ä¢ FraudAgent_eval-1767383707_events.jsonl\n",
      "  ‚Ä¢ FraudAgent_eval-1767304935_events.jsonl\n",
      "  ‚Ä¢ AuthAgent_eval-1767382480_events.jsonl\n",
      "  ‚Ä¢ FraudAgent_eval-1767304656_events.jsonl\n",
      "  ‚Ä¢ FraudAgent_eval-1767304660_events.jsonl\n",
      "  ‚Ä¢ FraudAgent_eval-1767304144_events.jsonl\n",
      "  ‚Ä¢ FraudAgent_eval-1767305848_events.jsonl\n",
      "  ‚Ä¢ FraudAgent_eval-1767304143_events.jsonl\n",
      "\n",
      "üìä Scoring: FraudAgent_eval-1767304143_events.jsonl\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-02 13:55:08,536] INFO - apps.artagent.backend.evaluation.scorer: Loaded 1 events from /Users/jinle/Repos/_AIProjects/art-voice-agent-accelerator/runs/jupyter_tests/FraudAgent_eval-1767304143_events.jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Scenario: jupyter_test\n",
      "Agent: FraudAgent\n",
      "Total Turns: 1\n",
      "\n",
      "üîß Tool Metrics:\n",
      "  Precision: 100.00%\n",
      "  Recall: 100.00%\n",
      "  Efficiency: 100.00%\n",
      "\n",
      "‚è±Ô∏è  Latency:\n",
      "  P50: 624.7ms\n",
      "  P95: 624.7ms\n",
      "\n",
      "üí∞ Cost:\n",
      "  Total tokens: 0\n",
      "  Estimated cost: $0.0000\n",
      "============================================================\n",
      "Scenario: jupyter_test\n",
      "Agent: FraudAgent\n",
      "Total Turns: 1\n",
      "\n",
      "üîß Tool Metrics:\n",
      "  Precision: 100.00%\n",
      "  Recall: 100.00%\n",
      "  Efficiency: 100.00%\n",
      "\n",
      "‚è±Ô∏è  Latency:\n",
      "  P50: 624.7ms\n",
      "  P95: 624.7ms\n",
      "\n",
      "üí∞ Cost:\n",
      "  Total tokens: 0\n",
      "  Estimated cost: $0.0000\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Find recorded events\n",
    "runs_dir = PROJECT_ROOT / \"runs\" / \"jupyter_tests\"\n",
    "\n",
    "if runs_dir.exists():\n",
    "    event_files = list(runs_dir.glob(\"*_events.jsonl\"))\n",
    "    \n",
    "    if event_files:\n",
    "        print(f\"üìÇ Found {len(event_files)} event file(s):\\n\")\n",
    "        for f in event_files:\n",
    "            print(f\"  ‚Ä¢ {f.name}\")\n",
    "        \n",
    "        # Score the most recent one\n",
    "        latest_events = event_files[-1]\n",
    "        print(f\"\\nüìä Scoring: {latest_events.name}\\n\")\n",
    "        \n",
    "        # Load and score\n",
    "        scorer = MetricsScorer()\n",
    "        events = scorer.load_events(latest_events)\n",
    "        \n",
    "        # Generate summary\n",
    "        summary = scorer.generate_summary(\n",
    "            events,\n",
    "            scenario_name=\"jupyter_test\",\n",
    "        )\n",
    "        \n",
    "        # Display summary\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Scenario: {summary.scenario_name}\")\n",
    "        print(f\"Agent: {summary.agent_name}\")\n",
    "        print(f\"Total Turns: {summary.total_turns}\")\n",
    "        print(f\"\\nüîß Tool Metrics:\")\n",
    "        print(f\"  Precision: {summary.tool_metrics['precision']:.2%}\")\n",
    "        print(f\"  Recall: {summary.tool_metrics['recall']:.2%}\")\n",
    "        print(f\"  Efficiency: {summary.tool_metrics['efficiency']:.2%}\")\n",
    "        print(f\"\\n‚è±Ô∏è  Latency:\")\n",
    "        print(f\"  P50: {summary.latency_metrics['e2e_p50_ms']:.1f}ms\")\n",
    "        print(f\"  P95: {summary.latency_metrics['e2e_p95_ms']:.1f}ms\")\n",
    "        print(f\"\\nüí∞ Cost:\")\n",
    "        total_tokens = summary.cost_analysis['total_input_tokens'] + summary.cost_analysis['total_output_tokens']\n",
    "        print(f\"  Total tokens: {total_tokens:,}\")\n",
    "        print(f\"  Estimated cost: ${summary.cost_analysis['estimated_cost_usd']:.4f}\")\n",
    "        print(\"=\" * 60)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No event files found. Run some tests first!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Runs directory not found. Run some tests first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interactive Widget for Testing\n",
    "\n",
    "A simple widget to test different agents and queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd09fb4a690e4b57b748d6d1b88ddd0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>üß™ Interactive Agent Tester</h3>'), Dropdown(description='Agent:', options=('Aut‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create interactive widgets (fresh instances each run to avoid handler accumulation)\n",
    "agent_selector = widgets.Dropdown(\n",
    "    options=sorted(list(agents.keys())),\n",
    "    description='Agent:',\n",
    "    style={'description_width': '100px'},\n",
    ")\n",
    "\n",
    "model_selector = widgets.Dropdown(\n",
    "    options=[\n",
    "        # GPT-4 family\n",
    "        'gpt-4o',\n",
    "        'gpt-4o-mini',\n",
    "        'gpt-4.1',\n",
    "        'gpt-4.1-mini',\n",
    "        'gpt-4.1-nano',\n",
    "        # GPT-5 family\n",
    "        'gpt-5',\n",
    "        'gpt-5-chat',\n",
    "        'gpt-5-mini',\n",
    "        # o-series (reasoning models)\n",
    "        'o1',\n",
    "        'o1-mini',\n",
    "        'o1-preview',\n",
    "        'o3',\n",
    "        'o3-mini',\n",
    "        'o4-mini',\n",
    "        'gpt-oss-120b'\n",
    "    ],\n",
    "    value='gpt-4o',\n",
    "    description='Model:',\n",
    "    style={'description_width': '100px'},\n",
    ")\n",
    "\n",
    "endpoint_selector = widgets.Dropdown(\n",
    "    options=['auto', 'chat', 'responses'],\n",
    "    value='auto',\n",
    "    description='Endpoint:',\n",
    "    style={'description_width': '100px'},\n",
    ")\n",
    "\n",
    "query_input = widgets.Textarea(\n",
    "    value='I need help with my account',\n",
    "    description='Query:',\n",
    "    style={'description_width': '100px'},\n",
    "    layout=widgets.Layout(width='500px', height='80px')\n",
    ")\n",
    "\n",
    "# Create fresh button each run (avoids accumulating click handlers)\n",
    "test_button = widgets.Button(\n",
    "    description='Run Test',\n",
    "    button_style='success',\n",
    "    icon='play',\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# Apply nest_asyncio once at module level\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def on_button_click(b):\n",
    "    \"\"\"Handle button click - run async test.\"\"\"\n",
    "    with output_area:\n",
    "        output_area.clear_output(wait=True)\n",
    "        print(\"‚è≥ Running test...\\n\")\n",
    "        \n",
    "        try:\n",
    "            # Get the current event loop and run the coroutine\n",
    "            loop = asyncio.get_event_loop()\n",
    "            result = loop.run_until_complete(run_single_turn(\n",
    "                agent_name=agent_selector.value,\n",
    "                user_query=query_input.value,\n",
    "                model_override={\n",
    "                    \"deployment_id\": model_selector.value,\n",
    "                    \"endpoint_preference\": endpoint_selector.value,\n",
    "                },\n",
    "            ))\n",
    "            \n",
    "            print(\"‚úÖ Test Complete\\n\")\n",
    "            print(f\"Agent: {result['agent']}\")\n",
    "            print(f\"Model: {result['model']} ({result['endpoint']})\\n\")\n",
    "            print(f\"Response:\\n{result['response']}\\n\")\n",
    "            print(f\"Tokens: {result['output_tokens']} | Latency: {result['latency_ms']:.0f}ms\")\n",
    "            \n",
    "            if result['error']:\n",
    "                print(f\"\\n‚ö†Ô∏è Error: {result['error']}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "# Register handler on fresh button\n",
    "test_button.on_click(on_button_click)\n",
    "\n",
    "# Build and display widget UI\n",
    "widget_ui = widgets.VBox([\n",
    "    widgets.HTML(\"<h3>üß™ Interactive Agent Tester</h3>\"),\n",
    "    agent_selector,\n",
    "    model_selector,\n",
    "    endpoint_selector,\n",
    "    query_input,\n",
    "    test_button,\n",
    "    output_area,\n",
    "])\n",
    "\n",
    "display(widget_ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load YAML Comparison\n",
    "\n",
    "Load the fraud detection comparison YAML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-02 13:55:08,625] INFO - apps.artagent.backend.evaluation.scenario_runner: Loading comparison from: /Users/jinle/Repos/_AIProjects/art-voice-agent-accelerator/tests/eval_scenarios/ab_tests/fraud_detection_comparison.yaml\n",
      "[2026-01-02 13:55:08,649] INFO - apps.artagent.backend.evaluation.scenario_runner: Loaded comparison: gpt4o_vs_o1_fraud\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded: gpt4o_vs_o1_fraud\n",
      "\n",
      "Variants:\n",
      "  ‚Ä¢ gpt4o_baseline\n",
      "    Model: gpt-4o\n",
      "    Endpoint: chat\n",
      "  ‚Ä¢ o3_mini_responses\n",
      "    Model: o3-mini\n",
      "    Endpoint: responses\n",
      "\n",
      "Note: To run this comparison with real orchestrators,\n",
      "      you'd need to implement the full comparison runner.\n",
      "      For now, use the compare_model_configs() function above.\n",
      "\n",
      "\n",
      "Variants:\n",
      "  ‚Ä¢ gpt4o_baseline\n",
      "    Model: gpt-4o\n",
      "    Endpoint: chat\n",
      "  ‚Ä¢ o3_mini_responses\n",
      "    Model: o3-mini\n",
      "    Endpoint: responses\n",
      "\n",
      "Note: To run this comparison with real orchestrators,\n",
      "      you'd need to implement the full comparison runner.\n",
      "      For now, use the compare_model_configs() function above.\n"
     ]
    }
   ],
   "source": [
    "# Load comparison YAML\n",
    "comparison_path = PROJECT_ROOT / \"tests\" / \"eval_scenarios\" / \"ab_tests\" / \"fraud_detection_comparison.yaml\"\n",
    "\n",
    "if comparison_path.exists():\n",
    "    runner = ComparisonRunner(\n",
    "        comparison_path=comparison_path,\n",
    "        output_dir=PROJECT_ROOT / \"runs\" / \"jupyter_comparison\",\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Loaded: {runner.comparison['comparison_name']}\")\n",
    "    print(f\"\\nVariants:\")\n",
    "    for variant in runner.comparison['variants']:\n",
    "        print(f\"  ‚Ä¢ {variant['variant_id']}\")\n",
    "        model = variant.get('model_override', {})\n",
    "        print(f\"    Model: {model.get('deployment_id')}\")\n",
    "        print(f\"    Endpoint: {model.get('endpoint_preference')}\")\n",
    "    \n",
    "    print(f\"\\nNote: To run this comparison with real orchestrators,\")\n",
    "    print(f\"      you'd need to implement the full comparison runner.\")\n",
    "    print(f\"      For now, use the compare_model_configs() function above.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Comparison YAML not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "You now have:\n",
    "\n",
    "‚úÖ **Real agent testing** - Not mocks, actual orchestrator\n",
    "‚úÖ **Event recording** - All turns recorded to JSONL\n",
    "‚úÖ **Metrics scoring** - Performance analysis\n",
    "‚úÖ **Model comparison** - Test different configs\n",
    "‚úÖ **Real-time optimization** - Best practices for minimal latency\n",
    "\n",
    "### Try These:\n",
    "\n",
    "1. **Test different agents**:\n",
    "   ```python\n",
    "   result = await run_single_turn(\n",
    "       agent_name=\"InvestmentAdvisor\",\n",
    "       user_query=\"How's my 401k doing?\",\n",
    "   )\n",
    "   ```\n",
    "\n",
    "2. **Compare endpoints**:\n",
    "   ```python\n",
    "   results = await compare_model_configs(\n",
    "       agent_name=\"FraudAgent\",\n",
    "       user_query=\"Suspicious charge\",\n",
    "       configs=[{...}, {...}],\n",
    "   )\n",
    "   ```\n",
    "\n",
    "3. **Analyze recorded events**:\n",
    "   - Events are saved to `runs/jupyter_tests/`\n",
    "   - Use MetricsScorer to analyze them\n",
    "\n",
    "4. **Optimize for real-time**:\n",
    "   - See the Real-Time Optimization section above\n",
    "   - Use `verbosity=0`, `reasoning_effort=\"low\"`, and capped tokens\n",
    "   - Test with the benchmark to measure improvements\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Evaluation Package](../../../apps/artagent/backend/evaluation/)\n",
    "- [Full Documentation](../../../docs/testing/model-evals.md)\n",
    "- [YAML Scenarios](../../../tests/eval_scenarios/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Running real-time optimization benchmark...\n",
      "\n",
      "üöÄ Benchmarking Real-Time Configurations\n",
      "\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Testing OPTIMAL configuration (verbosity=0, low reasoning, capped tokens)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-02 13:55:08,766] INFO - voice.shared.config_resolver: Checking for session-scoped scenario | session_id=eval-1767383708 scenario_name=None\n",
      "[2026-01-02 13:55:08,768] INFO - voice.shared.config_resolver: No stored scenarios for session | session_id=eval-1767383708\n",
      "[2026-01-02 13:55:08,837] INFO - cascade.adapter: üîß Agent tools loaded | agent=FraudAgent tool_count=12 tool_names=['analyze_recent_transactions', 'check_suspicious_activity', 'block_card_emergency', 'create_fraud_case', 'create_transaction_dispute', 'ship_replacement_card', 'send_fraud_case_email', 'provide_fraud_education', 'search_knowledge_base', 'transfer_call_to_call_center', 'escalate_emergency', 'escalate_human']\n",
      "[2026-01-02 13:55:08,877] INFO - cascade.adapter: Starting LLM request (streaming) | agent=FraudAgent model=gpt-5-mini temp=N/A iteration=0 tools=12\n",
      "[2026-01-02 13:55:08,926] ERROR - cascade.adapter: OpenAI stream error: Error code: 404 - {'error': {'code': 'DeploymentNotFound', 'message': 'The API deployment for this resource does not exist. If you created the deployment within the last 5 minutes, please wait a moment and try again.'}}\n",
      "[2026-01-02 13:55:08,928] ERROR - cascade.adapter: LLM processing failed: Error code: 404 - {'error': {'code': 'DeploymentNotFound', 'message': 'The API deployment for this resource does not exist. If you created the deployment within the last 5 minutes, please wait a moment and try again.'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö†Ô∏è  Testing SUBOPTIMAL configuration (verbosity=2, higher tokens)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-02 13:55:08,974] INFO - voice.shared.config_resolver: Checking for session-scoped scenario | session_id=eval-1767383708 scenario_name=None\n",
      "[2026-01-02 13:55:08,976] INFO - voice.shared.config_resolver: No stored scenarios for session | session_id=eval-1767383708\n",
      "[2026-01-02 13:55:09,016] INFO - cascade.adapter: üîß Agent tools loaded | agent=FraudAgent tool_count=12 tool_names=['analyze_recent_transactions', 'check_suspicious_activity', 'block_card_emergency', 'create_fraud_case', 'create_transaction_dispute', 'ship_replacement_card', 'send_fraud_case_email', 'provide_fraud_education', 'search_knowledge_base', 'transfer_call_to_call_center', 'escalate_emergency', 'escalate_human']\n",
      "[2026-01-02 13:55:09,055] INFO - cascade.adapter: Starting LLM request (streaming) | agent=FraudAgent model=gpt-5-mini temp=N/A iteration=0 tools=12\n",
      "[2026-01-02 13:55:09,102] ERROR - cascade.adapter: OpenAI stream error: Error code: 404 - {'error': {'code': 'DeploymentNotFound', 'message': 'The API deployment for this resource does not exist. If you created the deployment within the last 5 minutes, please wait a moment and try again.'}}\n",
      "[2026-01-02 13:55:09,105] ERROR - cascade.adapter: LLM processing failed: Error code: 404 - {'error': {'code': 'DeploymentNotFound', 'message': 'The API deployment for this resource does not exist. If you created the deployment within the last 5 minutes, please wait a moment and try again.'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "üìä REAL-TIME OPTIMIZATION RESULTS\n",
      "======================================================================\n",
      "\n",
      "‚úÖ OPTIMAL Configuration:\n",
      "   ‚Ä¢ Verbosity: 0 (minimal)\n",
      "   ‚Ä¢ Reasoning: low\n",
      "   ‚Ä¢ Max tokens: 2048\n",
      "   ‚Ä¢ Response length: 147 chars\n",
      "   ‚Ä¢ Output tokens: 0\n",
      "   ‚Ä¢ Latency: 206ms\n",
      "\n",
      "‚ö†Ô∏è  SUBOPTIMAL Configuration:\n",
      "   ‚Ä¢ Verbosity: 2 (detailed)\n",
      "   ‚Ä¢ Reasoning: default\n",
      "   ‚Ä¢ Max tokens: 8192\n",
      "   ‚Ä¢ Response length: 147 chars\n",
      "   ‚Ä¢ Output tokens: 0\n",
      "   ‚Ä¢ Latency: 196ms\n",
      "\n",
      "üéØ Performance Impact:\n",
      "   ‚Ä¢ Latency reduction: -10ms (-5.3% faster)\n",
      "   ‚Ä¢ Token efficiency: 0 vs 0 tokens\n",
      "\n",
      "üí° CONCLUSION: Both configs perform similarly (~-5.3% difference)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Real-time optimization comparison\n",
    "# Compare optimal vs suboptimal configurations\n",
    "\n",
    "import time\n",
    "\n",
    "async def benchmark_realtime_configs():\n",
    "    \"\"\"\n",
    "    Benchmark optimal vs suboptimal real-time configurations.\n",
    "    Shows the impact of proper configuration on latency.\n",
    "    \"\"\"\n",
    "    test_query = \"I see a suspicious charge on my account\"\n",
    "    \n",
    "    # ‚úÖ OPTIMAL Real-Time Configuration\n",
    "    optimal_config = {\n",
    "        \"deployment_id\": \"gpt-5-mini\",\n",
    "        \"endpoint_preference\": \"responses\",\n",
    "        \"verbosity\": 0,  # Minimal - fastest\n",
    "        \"reasoning_effort\": \"low\",  # Fast reasoning\n",
    "        \"max_completion_tokens\": 2048,  # Reasonable limit\n",
    "    }\n",
    "    \n",
    "    # ‚ö†Ô∏è SUBOPTIMAL Configuration (for comparison)\n",
    "    suboptimal_config = {\n",
    "        \"deployment_id\": \"gpt-5-mini\", \n",
    "        \"endpoint_preference\": \"responses\",\n",
    "        \"verbosity\": 2,  # Detailed - slower\n",
    "        # No reasoning_effort set - will use default \"medium\"\n",
    "        \"max_completion_tokens\": 8192,  # Excessive for real-time\n",
    "    }\n",
    "    \n",
    "    print(\"üöÄ Benchmarking Real-Time Configurations\\n\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Test 1: Optimal configuration\n",
    "    print(\"\\n‚úÖ Testing OPTIMAL configuration (verbosity=0, low reasoning, capped tokens)...\")\n",
    "    start = time.time()\n",
    "    result_optimal = await run_single_turn(\n",
    "        agent_name=\"FraudAgent\",\n",
    "        user_query=test_query,\n",
    "        model_override=optimal_config,\n",
    "        record_events=False,  # Skip recording for faster benchmark\n",
    "    )\n",
    "    optimal_latency = (time.time() - start) * 1000\n",
    "    \n",
    "    # Test 2: Suboptimal configuration  \n",
    "    print(f\"\\n‚ö†Ô∏è  Testing SUBOPTIMAL configuration (verbosity=2, higher tokens)...\")\n",
    "    start = time.time()\n",
    "    result_suboptimal = await run_single_turn(\n",
    "        agent_name=\"FraudAgent\",\n",
    "        user_query=test_query,\n",
    "        model_override=suboptimal_config,\n",
    "        record_events=False,\n",
    "    )\n",
    "    suboptimal_latency = (time.time() - start) * 1000\n",
    "    \n",
    "    # Display comparison\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä REAL-TIME OPTIMIZATION RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\n‚úÖ OPTIMAL Configuration:\")\n",
    "    print(f\"   ‚Ä¢ Verbosity: 0 (minimal)\")\n",
    "    print(f\"   ‚Ä¢ Reasoning: low\")\n",
    "    print(f\"   ‚Ä¢ Max tokens: 2048\")\n",
    "    print(f\"   ‚Ä¢ Response length: {len(result_optimal['response'])} chars\")\n",
    "    print(f\"   ‚Ä¢ Output tokens: {result_optimal['output_tokens']}\")\n",
    "    print(f\"   ‚Ä¢ Latency: {optimal_latency:.0f}ms\")\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  SUBOPTIMAL Configuration:\")\n",
    "    print(f\"   ‚Ä¢ Verbosity: 2 (detailed)\")\n",
    "    print(f\"   ‚Ä¢ Reasoning: default\")\n",
    "    print(f\"   ‚Ä¢ Max tokens: 8192\")\n",
    "    print(f\"   ‚Ä¢ Response length: {len(result_suboptimal['response'])} chars\")\n",
    "    print(f\"   ‚Ä¢ Output tokens: {result_suboptimal['output_tokens']}\")\n",
    "    print(f\"   ‚Ä¢ Latency: {suboptimal_latency:.0f}ms\")\n",
    "    \n",
    "    # Calculate improvement\n",
    "    improvement_pct = ((suboptimal_latency - optimal_latency) / suboptimal_latency) * 100\n",
    "    latency_diff = suboptimal_latency - optimal_latency\n",
    "    \n",
    "    print(f\"\\nüéØ Performance Impact:\")\n",
    "    print(f\"   ‚Ä¢ Latency reduction: {latency_diff:.0f}ms ({improvement_pct:.1f}% faster)\")\n",
    "    print(f\"   ‚Ä¢ Token efficiency: {result_optimal['output_tokens']} vs {result_suboptimal['output_tokens']} tokens\")\n",
    "    \n",
    "    if improvement_pct > 10:\n",
    "        print(f\"\\n‚úÖ CONCLUSION: Optimal configuration provides {improvement_pct:.1f}% better latency!\")\n",
    "        print(f\"   For real-time voice, this translates to noticeably faster responses.\")\n",
    "    else:\n",
    "        print(f\"\\nüí° CONCLUSION: Both configs perform similarly (~{improvement_pct:.1f}% difference)\")\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return {\n",
    "        \"optimal\": result_optimal,\n",
    "        \"suboptimal\": result_suboptimal,\n",
    "        \"improvement_pct\": improvement_pct,\n",
    "    }\n",
    "\n",
    "# Run the benchmark\n",
    "print(\"‚è≥ Running real-time optimization benchmark...\\n\")\n",
    "benchmark_results = await benchmark_realtime_configs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "art-voice-agent-accelerator (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
